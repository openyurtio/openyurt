apiVersion: v1
kind: Service
metadata:
  name: pool-coordinator-apiserver
  namespace: {{ .Release.Namespace | quote }}
  annotations:
    openyurt.io/topologyKeys: openyurt.io/nodepool
  labels:
    name: pool-coordinator
spec:
  type: ClusterIP
  ports:
    - port: 443
      targetPort: {{ .Values.poolCoordinator.apiserverSecurePort }}
      protocol: TCP
      name: https
  selector:
    k8s-app: pool-coordinator
---
apiVersion: v1
kind: Service
metadata:
  name: pool-coordinator-etcd
  namespace: {{ .Release.Namespace | quote }}
  annotations:
    openyurt.io/topologyKeys: openyurt.io/nodepool
  labels:
    name: pool-coordinator
spec:
  type: ClusterIP
  ports:
    - port: 2379
      targetPort: {{ .Values.poolCoordinator.etcdPort }}
      protocol: TCP
      name: https
  selector:
    k8s-app: pool-coordinator
---
#apiVersion: v1
#kind: ConfigMap
#metadata:
#  name: pool-coordinator-init-rbac
#  namespace: {{ .Release.Namespace | quote }}
#data:
#  pool-coordinator-init-rbac.yaml: |
#    apiVersion: rbac.authorization.k8s.io/v1
#    kind: ClusterRoleBinding
#    metadata:
#      name: openyurt:pool-coordinator:monitoring
#    roleRef:
#      apiGroup: rbac.authorization.k8s.io
#      kind: ClusterRole
#      name: openyurt:pool-coordinator:monitoring
#    subjects:
#    - apiGroup: rbac.authorization.k8s.io
#      kind: User
#      name: openyurt:pool-coordinator:monitoring
#    ---
#    apiVersion: rbac.authorization.k8s.io/v1
#    kind: ClusterRoleBinding
#    metadata:
#      name: openyurt:pool-coordinator:apiserver
#    roleRef:
#      apiGroup: rbac.authorization.k8s.io
#      kind: ClusterRole
#      name: openyurt:pool-coordinator:apiserver
#    subjects:
#    - apiGroup: rbac.authorization.k8s.io
#      kind: User
#      name: openyurt:pool-coordinator:apiserver
#    ---
#    apiVersion: rbac.authorization.k8s.io/v1
#    kind: ClusterRole
#    metadata:
#      name: openyurt:pool-coordinator:apiserver
#    rules:
#    - apiGroups: [""]
#      resources:  ["pods/attach", "pods/exec", "pods/portforward", "pods/proxy", "pods/log"]
#      verbs: ["get", "list"]
#    ---
#    apiVersion: rbac.authorization.k8s.io/v1
#    kind: ClusterRole
#    metadata:
#      name: openyurt:pool-coordinator:monitoring
#    rules:
#    - apiGroups: [""]
#      resources: [""]
#      verbs: ["get", "list", "watch"]
#---
apiVersion: apps.openyurt.io/v1alpha1
kind: YurtAppDaemon
metadata:
  name: pool-coordinator
  namespace: {{ .Release.Namespace | quote }}
spec:
  selector:
    matchLabels:
      k8s-app: pool-coordinator
  nodepoolSelector:
    matchLabels:
      openyurt.io/node-pool-type: "edge"
  workloadTemplate:
    deploymentTemplate:
      metadata:
        labels:
          k8s-app: pool-coordinator
      spec:
        replicas: 1
        selector:
          matchLabels:
            k8s-app: pool-coordinator
        template:
          metadata:
            labels:
              k8s-app: pool-coordinator
          spec:
            containers:
              - command:
                  - kube-apiserver
                  - --bind-address=0.0.0.0
                  - --allow-privileged=true
                  - --anonymous-auth=true
                  - --authorization-mode=Node,RBAC
                  - --client-ca-file=/etc/kubernetes/pki/ca.crt
                  - --enable-admission-plugins=NodeRestriction
                  - --enable-bootstrap-token-auth=true
                  - --disable-admission-plugins=ServiceAccount
                  - --etcd-cafile=/etc/kubernetes/pki/ca.crt
                  - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
                  - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
                  - --etcd-servers=http://127.0.0.1:{{ .Values.poolCoordinator.etcdPort }}
                  - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
                  - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
                  - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
                  - --secure-port={{ .Values.poolCoordinator.apiserverSecurePort }}
                  - --service-account-issuer=https://kubernetes.default.svc.cluster.local
                  - --service-account-key-file=/etc/kubernetes/pki/sa.pub
                  - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
                  - --service-cluster-ip-range={{ .Values.poolCoordinator.serviceClusterIPRange }}
                  - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
                  - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
                image: "{{ .Values.poolCoordinator.apiserverImage.registry }}/{{ .Values.poolCoordinator.apiserverImage.repository }}:{{ .Values.poolCoordinator.apiserverImage.tag }}"
                imagePullPolicy: {{ .Values.poolCoordinator.apiserverImage.pullPolicy }}
                {{- if .Values.imagePullSecrets }}
                imagePullSecrets: {{ toYaml .Values.imagePullSecrets | nindent 8 }}
                {{- end }}
                livenessProbe:
                  failureThreshold: 8
                  httpGet:
                    host: 127.0.0.1
                    path: /livez
                    port: {{ .Values.poolCoordinator.apiserverSecurePort }}
                    scheme: HTTPS
                  initialDelaySeconds: 10
                  periodSeconds: 10
                  successThreshold: 1
                  timeoutSeconds: 15
                name: kube-apiserver
                readinessProbe:
                  failureThreshold: 3
                  httpGet:
                    host: 127.0.0.1
                    path: /readyz
                    port: {{ .Values.poolCoordinator.apiserverSecurePort }}
                    scheme: HTTPS
                  periodSeconds: 1
                  successThreshold: 1
                  timeoutSeconds: 15
                resources:
                  {{- toYaml .Values.poolCoordinator.apiserverResources | nindent 12 }}
                startupProbe:
                  failureThreshold: 24
                  httpGet:
                    host: 127.0.0.1
                    path: /livez
                    port: {{ .Values.poolCoordinator.apiserverSecurePort }}
                    scheme: HTTPS
                  initialDelaySeconds: 10
                  periodSeconds: 10
                  successThreshold: 1
                  timeoutSeconds: 15
                terminationMessagePath: /dev/termination-log
                terminationMessagePolicy: File
                volumeMounts:
                  - mountPath: /etc/kubernetes/pki
                    name: dynamic-certs
                    readOnly: true
                  - mountPath: /etc/kubernetes/pki
                    name: static-certs
                    readOnly: true
              - command:
                  - etcd
                  - --advertise-client-urls=https://0.0.0.0:{{ .Values.poolCoordinator.etcdPort }}
                  - --listen-client-urls=https://0.0.0.0:{{ .Values.poolCoordinator.etcdPort }}
                  - --cert-file=/etc/kubernetes/pki/etcd-server.crt
                  - --client-cert-auth=true
                  - --data-dir=/var/lib/etcd
                  - --key-file=/etc/kubernetes/pki/etcd-server.key
                  - --listen-metrics-urls=http://0.0.0.0:{{ .Values.poolCoordinator.etcdMetricPort }}
                  - --snapshot-count=10000
                  - --trusted-ca-file=/etc/kubernetes/pki/ca.crt
                image: "{{ .Values.poolCoordinator.etcdImage.registry }}/{{ .Values.poolCoordinator.etcdImage.repository }}:{{ .Values.poolCoordinator.etcdImage.tag }}"
                imagePullPolicy: {{ .Values.poolCoordinator.etcdImage.pullPolicy }}
                {{- if .Values.imagePullSecrets }}
                imagePullSecrets: {{ toYaml .Values.imagePullSecrets | nindent 8 }}
                {{- end }}
                name: etcd
                resources:
                  {{- toYaml .Values.poolCoordinator.etcdResources | nindent 12 }}
                startupProbe:
                  failureThreshold: 24
                  httpGet:
                    host: 127.0.0.1
                    path: /health
                    port: {{ .Values.poolCoordinator.etcdMetricPort }}
                    scheme: HTTP
                  initialDelaySeconds: 10
                  periodSeconds: 10
                  successThreshold: 1
                  timeoutSeconds: 15
                volumeMounts:
                  - mountPath: /var/lib/etcd
                    name: etcd-data
                  - mountPath: /etc/kubernetes/pki
                    name: dynamic-certs
                    readOnly: true
                  - mountPath: /etc/kubernetes/pki
                    name: static-certs
                    readOnly: true
#              - image: "{{ .Values.poolCoordinator.kubectlImage.registry }}/{{ .Values.poolCoordinator.kubectlImage.repository }}:{{ .Values.poolCoordinator.kubectlImage.tag }}"
#                imagePullPolicy: {{ .Values.poolCoordinator.apiserverImage.pullPolicy }}
#                {{- if .Values.imagePullSecrets }}
#                imagePullSecrets: {{ toYaml .Values.imagePullSecrets | nindent 8 }}
#                {{- end }}
#                  lifecycle:
#                    postStart:
#                      exec:
#                        command:
#                          - kubectl
#                          - apply
#                          - -f
#                          - /etc/kubernetes/pool-coordinator-init-rbac.yaml
#                  name: kubectl
#                  resources:
#                    {{- toYaml .Values.poolCoordinator.etcdResources | nindent 12 }}
#                  securityContext:
#                    privileged: true
#                  terminationMessagePath: /dev/termination-log
#                  terminationMessagePolicy: File
#                  tty: true
#                  volumeMounts:
#                    - mountPath: /root/.kube/
#                      name: kubeconfig
#                    - mountPath: /etc/kubernetes/pool-coordinator-init-rbac.yaml
#                      name: pool-coordinator-init-rbac
            dnsPolicy: ClusterFirst
            enableServiceLinks: true
            hostNetwork: true
            preemptionPolicy: PreemptLowerPriority
            priority: 2000001000
            priorityClassName: system-node-critical
            restartPolicy: Always
            schedulerName: default-scheduler
            securityContext:
              seccompProfile:
                type: RuntimeDefault
            terminationGracePeriodSeconds: 30
            tolerations:
              - effect: NoExecute
                operator: Exists
            volumes:
              - emptyDir:
                  medium: Memory
                name: etcd-data
              - secret:
                  secretName: pool-coordinator-dynamic-certs
                  defaultMode: 420
                name: dynamic-certs
              - secret:
                  secretName: pool-coordinator-static-certs
                  defaultMode: 420
                name: static-certs
#              - secret:
#                  secretName: pool-coordinator-self-kubeconfig
#                  defaultMode: 420
#                name: kubeconfig
#              - configMap:
#                  name: pool-coordinator-init-rbac
#                name: pool-coordinator-init-rbac
